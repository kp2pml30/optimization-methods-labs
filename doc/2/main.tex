\documentclass[russian, english]{article}

% \usepackage[margin=1in]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{csvsimple}
\usepackage{hyperref}
\usepackage[pdf]{graphviz}
\usepackage[final]{pdfpages}

\inputencoding{utf8}
\def\code#1{\texttt{#1}}

\begin{document}

\begin{titlepage}
\centering
	{\scshape\LARGE Методы оптимизации \par}
	\vspace{1cm}
	{\scshape\Large Лабораторная работа №2\par}
	\vspace{2cm}
	{\Large\itshape Дмитрий Проценко M3234 \par
	Кирилл Прокопенко M3236 \par
	Николай Холявин M3238 \par}
	\vfill
	ИТМО y2019
	\vfill
	{\large \today\par}
\end{titlepage}

\tableofcontents
\newpage

\section{Цель}
Исследовать методы:
\begin{itemize}
	\item градиентного спуска
	\item наискорейшего спуска
	\item сопряженных градиентов
\end{itemize}
Проанализировать:
\begin{itemize}
	\item зависимость скорость сходимости от одномерной оптимизации
	\item траектории методов
	\item зависимость числа итераций от размерности пространства, от числа обусловленности
\end{itemize}

\section{Зависимость скорости сходимости от одномерного поиска}
Мы определили скорость сходимости как число шагов градиентного метода, значит, если мы находим минимум с заданной точностью, то это не имеет влияния на результат (в рамках пограшености)

\section{Примеры квадратичных функций}
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{1.png}}
\newpage
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{2.png}}
\newpage
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{3.png}}
\par
Видно, что все методы, стартуя из одной точки, идут в одном направлении (антиградиента).
Метод градиентного спуска каждый раз ``перепрыгивает'' минимум по направлению (для второй функции более наглядно: и сам минимум), поскольку его фиксированный шаг больше.
Метод сопряженных градиентов находит в 2 шага, поскольку это верхняя граница числа его итераций --- размерность пространства.
Метод наискорейшего спуска каждый раз находит минимум по направлению, что видболее наглядно: но на картинке.
\section{Зависимость числа итераций}
\subsection{Параметры}
\csvautotabular[separator=tab]{properties.tsv}

\subsection{Графики}
``Шумные'' падения на графиках свзяаны с тем, что иногда может ``повезти'' и антиградиент приблизит к минимуму сильнее, чем в среднем.\\

\def\makePlots#1#2{
	\subsubsection{#2}
	\pgfplotstableread{#1}{\ratiosCsv}
	\begin{tikzpicture}[trim axis left]
		\begin{axis}[
			scale only axis,
			width=\textwidth,
			legend style={at={(0.5,-0.1)},anchor=north},
			xlabel = {число обусловленности},
			ylabel = {итерация},
		]
			\foreach \ind in {10, 100, 1000, 10000}{
				\edef\temp{\noexpand\addlegendentry{$n=\ind$}}
				\addplot table [x={cond}, y={n=\ind}] {\ratiosCsv};
				\temp
			}
		\end{axis}
	\end{tikzpicture}
}

\makePlots{gradient-descent.tsv}{Градиентный спуск}
\\
Демонстрируется ``линейная сходимость''.
\newpage
\makePlots{steepest-descent.tsv}{Наискорейший спуск}
\\
Лучше, чем градиентный спуск, но сходимость все еще линейная.
\newpage
\makePlots{conjugate-gradient-descent.tsv}{Метод сопряженных градиентов}
\\
Изученная теоеория утверждает, что сходится не более чем за константное число шагов для квадратичных функций (равное $n$), что подтвердилось экспериментами (для $n=10$)

\newpage
\section{Вывод}
В ходе работы была реализована библиотека методов многомерной оптимизации на основе кода прошлой лабораторной работы, разработан пользовательский интерфейс, исследована зависимость числа итераций от размерности пространства и числа обусловленности. Полученные результаты были обоснованы на основании изученной теории. \\
Метод сопряженных градиентов имеет меньшее число итераций на исследованных функциях по сравнению с другими методами.

\appendix
\section{Код}
Основан на первой лабораторной, исправлены некоторые архитекнутрные недостатки.

\section{Диаграмма классов}
Из-за использования концептов doxygen не сгенерирует адекватную диаграмму. Граф, сгенерированный вручную, представлен ниже. \\
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{project.pdf}}
%\setboolean{@twoside}{false}
%\includepdf[pages=-,pagecommand={},width=\paperwidth]{project.pdf}

\section{FAQ}
\def\Question#1#2{
	\item\begin{itemize}
		\item[В] #1
		\item[О] #2
	\end{itemize}
}
\begin{itemize}
	\Question{Чему равны градиент и гессиан квадратичной функции?}{$(\frac{1}{2}Ax^2+bx+c=0)' = Ax + b$\\ $(Ax + b)' = A$}
	\Question{Каким свойством обладает квадратичная функция с положительно определенной матрицей $A$?}{Имеет глобальный минимум, выпуклая}
	\Question{Чем можете охарактеризовать собственный вектор матрицы $А$ квадратичной функции?}{Собственным числом!!!}
	\Question{Когда говорят, что в итерационном процессе производится исчерпывающий спуск?}{Спуск до миниума по направлению (решение одномерной оптимизации)}
	\Question{Какие направления дифференцируемой в точке функции называются направлениями убывания? Каков геометрический смысл направления убывания?}{Точка $S$, направление $D$. Существует окрестность в которой функция меньше, т.е. $\exists E > 0: \forall\varepsilon \le E: f(S+D\cdot\varepsilon)\le f(S)$}
		\Question{Какова скорость сходимости метода градиентного спуска для квадратичной функции с положительно определенной симметричной матрицей $A$, где $l$ и $L$ --- ее наименьшее и наибольшее собственные значения?}{Линейна по $\frac{L}{l}$}
	\Question{Когда говорят, что сильно выпуклая функция имеет овражный характер? Какие задачи минимизации называются хорошо обусловленными, а какие --- плохо обусловленными?}{Овражные функции в 3-х мерном пространстве визуально напоминают овраг: изменение по 1 направлению намного больше, чем по другому и имеет минимум. Тогда метод оптимизации может ``перепрыгивать'' через этот овраг, вместо того чтобы искать минимум на ``дне''. Хорошо обусловленная значит, что ``малое'' изменение коэффициентов повлечет ``малые'' изменения в решении. Некая ``мера'' ошибки функции от входных данных.}
	\Question{В чем состоят преимущества и недостатки метода наискорейшего спуска по сравнению с методом градиентного спуска?}{Идет в "идеальную" точку по направлению, не перескочит минимум, но нужна одномерная оптимизация, которая много раз будет вычислять изначальную функцию, что может быть недопустимо.}
	\Question{Каков главный недостаток градиентных методов?}{Находят локальный минимум, требуют производную}
	\Question{В чем состоит идея метода сопряженных градиентов? Чем этот метод отличается от методов градиентного и наискорейшего спуска?}{Изначально рассчитан на квадратичные функций, но обощается}
\end{itemize}

\end{document}
