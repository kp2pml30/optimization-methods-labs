\documentclass[russian, english]{article}

% \usepackage[margin=1in]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{csvsimple}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage[pdf]{graphviz}
\usepackage[final]{pdfpages}

\inputencoding{utf8}
\def\code#1{\texttt{#1}}

\begin{document}

\begin{titlepage}
\centering
	{\scshape\LARGE Методы оптимизации \par}
	\vspace{1cm}
	{\scshape\Large Лабораторная работа №2\par}
	\vspace{2cm}
	{\Large\itshape Дмитрий Проценко M3234 \par
	Кирилл Прокопенко M3236 \par
	Николай Холявин M3238 \par}
	\vfill
	ИТМО y2019
	\vfill
	{\large \today\par}
\end{titlepage}

\tableofcontents
\newpage

\section{Цель}
Исследовать методы:
\begin{itemize}
	\item градиентного спуска
	\item наискорейшего спуска
	\item сопряженных градиентов
\end{itemize}
Проанализировать:
\begin{itemize}
	\item зависимость скорость сходимости от одномерной оптимизации
	\item траектории методов
	\item зависимость числа итераций от размерности пространства, от числа обусловленности
\end{itemize}

\section{Вычислительная схема методов}
\begin{itemize}
\item Градиентный спуск:\\
$x^{k+1}=x^k-\alpha\cdot\nabla f(x)$, где на каждой итерации $\alpha$ уменьшается в 2 раза, пока не выполнится условие $f(x^{k+1}) < f(x^k)$; вначале $\alpha > 0$ выводится из условий задачи.
\item Наискорейший спуск:\\
$x^{k+1}=x^k-\alpha\cdot\nabla f(x)$, где на каждой итерации $\alpha$ находится из решения задачи одномерной оптимизации функции $\Phi_k(\alpha) = f(x^k - \alpha\cdot\nabla f(x^k))$ на отрезке от 0 до заданной величины.
\item Метод сопряжённых градиентов (применим только для квадратичных функций):\\
$x^{k+1}=x^k+\alpha_k\cdot p^k$, где $p^0 = -\nabla f(x^0); \; p^{k+1}=-\nabla f(x^{k+1})+\beta_k\cdot p^k$ и $\beta_k = \frac{\left(A \nabla f(x^{k+1}), p^k\right)}{\left(Ap^k, p^k\right)}$ и $\alpha_k=-\frac{\left(\nabla f(x^k), p^k\right)}{\left(Ap^k, p^k\right)}.$
\end{itemize}

\section{Зависимость скорости сходимости от одномерного поиска}
Мы определили скорость сходимости как число шагов градиентного метода, значит, если мы находим минимум с заданной точностью, то это не имеет влияния на результат (в рамках погрешности). Однако метод парабол может не выдать ответа, что может отрицательно повлиять на скорость сходимости.

\section{Примеры квадратичных функций}
Начальная точка~--- $(1;1;\dots;1)$.
\par
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{1.png}}
\newpage
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{2.png}}
\newpage
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{3.png}}
\par
Видно, что все методы, стартуя из одной точки, идут в одном направлении (антиградиента). \\
Метод градиентного спуска каждый раз ``перепрыгивает'' минимум по направлению (для второй функции более наглядно: и сам минимум), поскольку его фиксированный шаг больше. \\
Метод сопряженных градиентов находит в 2 шага, поскольку это верхняя граница числа его итераций --- размерность пространства. \\
Метод наискорейшего спуска каждый раз находит минимум по направлению, попадая в очередную линию уровня по касательной, что видно на картинке.
\section{Зависимость числа итераций}
\subsection{Параметры}
\csvautotabular[separator=tab]{properties.tsv}

Входными данными для алгоритмов являлись квадратичные функции с диагональной матрицей, составленной из строго возрастающей последовательности чисел от 1 до $k$~--- заданного числа обусловленности. Начальная $\alpha=\frac{2}{1+k}$ для методов наискорейшего (длина отрезка одномерной минимизации) и градиентного спуска.

\subsection{Графики}

\def\makePlots#1#2{
	\subsubsection{#2}
	\pgfplotstableread{#1}{\ratiosCsv}
	\begin{tikzpicture}[trim axis left]
		\begin{axis}[
			scale only axis,
			width=\textwidth,
			legend style={at={(0.5,-0.1)},anchor=north},
			xlabel = {число обусловленности},
			ylabel = {итерация},
		]
			\foreach \ind in {10, 100, 1000, 10000}{
				\edef\temp{\noexpand\addlegendentry{$n=\ind$}}
				\addplot table [x={cond}, y={n=\ind}] {\ratiosCsv};
				\temp
			}
		\end{axis}
	\end{tikzpicture}
}

\makePlots{gradient-descent.tsv}{Градиентный спуск}
\\
Демонстрируется ``линейная сходимость''. При описанных начальных данных погрешность по каждой координате на $i$-й итерации оценивается как $c_1\cdot(1-\frac{c_2}{k})^i \sim c_1\cdot(1-i\frac{c_2}{k})$, что приближает константную точность с числом $i$, линейно зависящим от $k$.
\newpage
\makePlots{steepest-descent.tsv}{Наискорейший спуск}
\\
Лучше, чем градиентный спуск, но сходимость все еще линейная. Для двумерного случая можно это обосновать так: одномерная оптимизация по прямой $l$ выбирает эллипс линии уровня так, чтобы в точке пересечения $l$ и эллипса касательная совпадала с $l$ (линия наименьшего уровня). При увеличивании $k$ эллипсы вместе с касательными растягиваются, и нужно линейно большее число шагов для достижения нуля.
\newpage
\makePlots{conjugate-gradient-descent.tsv}{Метод сопряженных градиентов}
\\
Изученная теория утверждает, что сходится не более чем за константное число шагов для квадратичных функций (равное $n$), что подтвердилось экспериментами (для $n=10$). Можно показать, что теоретическую границу количества итераций для достаточно больших $k$ можно улучшить как $\mathcal{O}(\sqrt{k})$, что видно на графике (лдя $n=100,1000,10000$).

\newpage
\section{Графический интерфейс}
Для отрисовки графиков с линиями уровня функций и траекториями методов был разработан и реализован графический интерфейс на основе графической библиотеки Qt с использованием языка программирования C++. Графический интерфейс также включает в себя возможность выбора точности, методов многомерной и одномерной оптимизации, а также отображение минимизируемой функции:\\
\noindent\makebox[\textwidth]{\includegraphics[scale=0.7]{gui_1.png}}

Полотно для изображения самого графика включает в себя легенду и сам график. В процессе реализации легенды для user-friendly отображения функции был написан алгоритм, облегчающий восприятие коэффициентов. Есть возможность масштабирования и базовой навигации по изображению. Кроме графика самой функции, также отображены линии уровня выбранной функции (с соответствующих итераций метода оптимизации или самой функции в автоматически определённых точках, если никакой метод не выбран) и траектории выбранных методов:\\
\noindent\makebox[\textwidth]{\includegraphics[scale=0.4]{gui_2.png}}

\newpage
\section{Вывод}
В ходе работы была реализована библиотека методов многомерной оптимизации на основе кода прошлой лабораторной работы, разработан пользовательский интерфейс, исследована зависимость числа итераций от размерности пространства и числа обусловленности. Полученные результаты были обоснованы на основании изученной теории. \\
Метод сопряженных градиентов имеет меньшее число итераций на исследованных функциях по сравнению с другими методами.

\appendix
\section{Код}
Основан на первой лабораторной, исправлены некоторые архитектурные недостатки.

\section{Диаграмма классов}
Из-за использования концептов doxygen не сгенерирует адекватную диаграмму. Граф, сгенерированный вручную, представлен ниже. \\
\noindent\makebox[\textwidth]{\includegraphics[width=\paperwidth]{project.pdf}}
%\setboolean{@twoside}{false}
%\includepdf[pages=-,pagecommand={},width=\paperwidth]{project.pdf}

\begin{comment}
\section{FAQ}
\def\Question#1#2{
	\item\begin{itemize}
		\item[В] #1
		\item[О] #2
	\end{itemize}
}
\begin{itemize}
	\Question{Чему равны градиент и гессиан квадратичной функции?}{$(\frac{1}{2}Ax^2+bx+c=0)' = Ax + b$\\ $(Ax + b)' = A$}
	\Question{Каким свойством обладает квадратичная функция с положительно определенной матрицей $A$?}{Имеет глобальный минимум, выпуклая}
	\Question{Чем можете охарактеризовать собственный вектор матрицы $А$ квадратичной функции?}{Собственным числом!!!}
	\Question{Когда говорят, что в итерационном процессе производится исчерпывающий спуск?}{Спуск до минимума по направлению (решение одномерной оптимизации)}
	\Question{Какие направления дифференцируемой в точке функции называются направлениями убывания? Каков геометрический смысл направления убывания?}{Точка $S$, направление $D$. Существует окрестность в которой функция меньше, т.е. $\exists E > 0: \forall\varepsilon \le E: f(S+D\cdot\varepsilon)\le f(S)$}
		\Question{Какова скорость сходимости метода градиентного спуска для квадратичной функции с положительно определенной симметричной матрицей $A$, где $l$ и $L$ --- ее наименьшее и наибольшее собственные значения?}{Линейна по $\frac{L}{l}$}
	\Question{Когда говорят, что сильно выпуклая функция имеет овражный характер? Какие задачи минимизации называются хорошо обусловленными, а какие --- плохо обусловленными?}{Овражные функции в 3-х мерном пространстве визуально напоминают овраг: изменение по 1 направлению намного больше, чем по другому и имеет минимум. Тогда метод оптимизации может ``перепрыгивать'' через этот овраг, вместо того чтобы искать минимум на ``дне''. Хорошо обусловленная значит, что ``малое'' изменение коэффициентов повлечет ``малые'' изменения в решении. Некая ``мера'' ошибки функции от входных данных.}
	\Question{В чем состоят преимущества и недостатки метода наискорейшего спуска по сравнению с методом градиентного спуска?}{Идет в "идеальную" точку по направлению, не перескочит минимум, но нужна одномерная оптимизация, которая много раз будет вычислять изначальную функцию, что может быть недопустимо.}
	\Question{Каков главный недостаток градиентных методов?}{Находят локальный минимум, требуют производную}
	\Question{В чем состоит идея метода сопряженных градиентов? Чем этот метод отличается от методов градиентного и наискорейшего спуска?}{Изначально рассчитан на квадратичные функций, но обобщается}
\end{itemize}
\end{comment}

\end{document}
